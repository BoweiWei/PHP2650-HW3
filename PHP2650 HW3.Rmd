---
title: "PHP2650 HW3"
author: "Bowei Wei"
date: "4/1/2018"
output: html_document
---

This assignment also means to illustrate the complexity in real-world practice. One may need to integrate in modeling multiple sources of data, scattered in multiple tables. In fact, many enterprise SQL databases usually contain many tables, and thatâ€™s the usual way how data are organized to optimize logistic flow and storage efficiency. The complication for modeling is that these tables do not necessarily contain the same IDs, and some will be missing in one or a few tables.

The basic dataset is in a sqlite database: pred.sqlite on Canvas. This dataset contains 3 tables: pred, demo, and outcome. The first column of each table is the unique ID for each subject, and the remaining collumns are variables that be used for model building. The practical goal is to predict cognitive scores (in the outcome table) using other variables.
```{r}
library(RSQLite)
library(dplyr)
library(methods)
library(glmnet)
## connect to db
con <- dbConnect(drv=RSQLite::SQLite(), dbname="pred.sqlite")

## list all tables
tables <- dbListTables(con)

## exclude sqlite_sequence (contains table information)
tables <- tables[tables != "sqlite_sequence"]

lDataFrames <- vector("list", length=length(tables))

## create a data.frame for each table
for (i in seq(along=tables)) {
  lDataFrames[[i]] <- dbGetQuery(conn=con, statement=paste("SELECT * FROM '", tables[[i]], "'", sep=""))
}

## name the tables
demo <- lDataFrames[[1]]
outcome <- lDataFrames[[2]]
pred <- lDataFrames[[3]]
```
[10%] Find the IDs that are available in all tables, and the IDs that available only in Table pred and demo but not in Table outcome.
Output two csv files: idall.csv should contain a vector of IDs in all tables, and idmissing.csv should contain a vector of IDs that are missing in Table outcome.
```{r}
## find all id and save as csv
idall <- inner_join(demo, pred, by = "id")
idall <- inner_join(idall, outcome, by = "id")
write.csv(idall, "idall.csv")
idall.na <- na.omit(idall)
idall$gender <- as.factor(idall$gender)
idall$age <- as.factor(idall$age)

## find all id that contained in pred and demo but not in outcome and save as csv file
idpred.demo <- inner_join(pred, demo, by = "id")
idmissing.id <- anti_join(idpred.demo, outcome, by = "id")

idmissing <- idmissing.id[,-1]
idmissing <- idmissing[,c(106:107,1:105)]
idmissing <- model.matrix(~ .,idmissing)[,-1]
write.csv(idmissing, "idmissing.csv")
idob <- idall[, 109:126]

q2 <- idall[,2:108]
```
[30%] Build a model for predicting the first outcome score (the second column in Table outcome), using the data from the first ID list in Problem 1. Generate the corresponding predicted values using your model for the missing IDs in Problem 1.
Output one csv file: output1.csv should contain one matrix element. The first column of this matrix is the second ID list from Problem 1 and the second column is the predicted values generated by your model. We will compare your predicted values with the measured values withheld from you, using the measn squared error loss.
```{r}
## ridge regression:
X = model.matrix( ~ ., q2)[, -1]
Y = idob$O1

fit_ridge = glmnet(X, Y, alpha = 0)
plot(fit_ridge)
plot(fit_ridge, xvar = "lambda", label = TRUE)
dim(coef(fit_ridge))

fit_ridge_cv = cv.glmnet(X, Y, alpha = 0)
plot(fit_ridge_cv)

coef(fit_ridge_cv)

coef(fit_ridge_cv, s = "lambda.min")
sum(coef(fit_ridge_cv, s = "lambda.min")[-1] ^ 2) # penalty term for lambda minimum
coef(fit_ridge_cv, s = "lambda.1se")

sum(coef(fit_ridge_cv, s = "lambda.1se")[-1] ^ 2) # penalty term for lambda one SE
mean((Y - predict(fit_ridge_cv, X)) ^ 2) # "train error"

sqrt(fit_ridge_cv$cvm) # CV-RMSEs
sqrt(fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min]) # CV-RMSE minimum
sqrt(fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.1se]) # CV-RMSE one SE

```
```{r}
## lasso regression:
fit_lasso = glmnet(X, Y, alpha = 1)
plot(fit_lasso)

plot(fit_lasso, xvar = "lambda", label = TRUE)

dim(coef(fit_lasso))

fit_lasso_cv = cv.glmnet(X, Y, alpha = 1)
plot(fit_lasso_cv)


fit = glmnet(X, Y, alpha = 1, lambda = fit_lasso_cv$lambda.min)

coef(fit_lasso_cv, s = "lambda.min")

sum(abs(coef(fit_lasso_cv, s = "lambda.min")[-1])) # penalty term for lambda minimum

coef(fit_lasso_cv, s = "lambda.1se")

sum(abs(coef(fit_lasso_cv, s = "lambda.1se")[-1])) # penalty term for lambda one SE

mean((Y - predict(fit_lasso_cv, X)) ^ 2) # "train error"

sqrt(fit_lasso_cv$cvm)

sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min]) # CV-RMSE minimum

sqrt(fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.1se]) # CV-RMSE one SE

fit_lasso_final = glmnet(X, Y, alpha = 1, lambda = fit_lasso_cv$lambda.min)

predict(fit_lasso_final, idmissing, type = "response")

OB <- data.frame(matrix(NA, nrow = 303, ncol = 18))
colnames(OB) <- paste("O",1:18, sep ="")
total <- 0

Opred <- function(X, Y, i){
  set.seed(10)
  fit.lasso <- cv.glmnet(X, Y, alpha = 1)
  fit.1 <- glmnet(X, Y, alpha = 1, lambda = fit.lasso$lambda.min)
  x<- predict(fit.1, idmissing, type = "response")
  a <- min(fit.lasso$cvm)
  list(cvm = a, predi = x)
}

for (i in 1:ncol(OB)) {
  out <- Opred(X, idob[, i], i)
  OB[,i] <- out$predi
  total <- total + out$cvm
}
```

```{r}
## regular linear regression:
fit <- lm(V1 ~ ., idall)
coef(fit)
sum(abs(coef(fit)[-1]))
sum(coef(fit)[-1]^2)

## 
```

```{r}


```

[30%] Similar to problem 2, generate predicted values for all outcome columns in the Table outcome, including the first column that you did in Problem 2.
Output one csv file: output2.csv should contain one matrix element. The first column of this matrix is the second ID list from Problem 1, and the remaining columns are the predicted values generated by your model, by the same order as in Table outcome.
```{r}
X1 = model.matrix(as.matrix(idall[,109:126]) ~ .,q2)[, -1]
Y1 = as.matrix(idall[,109:126])
idmissing.1 <- 

m.fit = cv.glmnet(X1, Y1, family = "mgaussian", alpha = 1)
plot(m.fit)



```
[30%] Do more data help improve the prediction performance? Additional predictors are available from the CSV file pred2.csv on Canvas. The first column is the IDs and the remainding ones are additional predictors. Carry out similar steps as before to build your predictive models for all outcomes, using these additional predictors.
Output one csv file: output3.csv should contain one matrix element, similar to Problem 3, for predicting the multiple scores.
```{r}
## find the new idall with more predictors
pred2 <- read.csv("pred2.csv", header = FALSE, sep = ",")
colnames(pred2) <- c("id", paste("V",106:304, sep =""))
idall.1 <- inner_join(idall.1, pred2, by = "id")
idall.1 <- idall.1[,c(1:108, 127:325, 109:126)]

OB1 <- data.frame(matrix(NA, nrow = 303, ncol = 18))
colnames(OB1) <- paste("O",1:18, sep ="")
total1 <- 0

## find the new independent variable X1 and dependent Y1
q3 <- idall.1[,2:307]
X1 <- model.matrix(~. , q3)[, -1]
idmissing.1.id <- left_join(idmissing.id, pred2, by = "id")
idmissing.1 <- idmissing.1.id[,c(107:108,2:106,109:307)]
idmissing.1 <- model.matrix(~ .,idmissing.1)[,-1]

Opred.1 <- function(X, Y, i){
  set.seed(10)
  fit.lasso <- cv.glmnet(X, Y, alpha = 1)
  fit.1 <- glmnet(X, Y, alpha = 1, lambda = fit.lasso$lambda.min)
  x <- predict(fit.1, idmissing.1, type = "response")
  a <- min(fit.lasso$cvm)
  list(cvm = a, predi = x)
}

for (i in 1:ncol(OB1)) {
  out <- Opred.1(X1, idob[, i], i)
  OB1[,i] <- out$predi
  total1 <- total1 + out$cvm
}

```
[Bonus 30%] Can a really large number of predictors help improve the prediction accuracy? Similar to Problem 4, please consider including additional predictors from pred3.csv from https://www.dropbox.com/s/8pv2z9l4bccityn/pred3.csv?dl=0 or here
Output one csv file: output4.csv should contain one matrix element, similar to Problem 4, for predicting the multiple scores using this larger dataset.
```{r}
pred3 <- read.csv("pred3.csv", header = FALSE, sep = ",")
colnames(pred3) <- c("id", paste("V",305:45154, sep =""))
idall.2 <- inner_join(idall.1, pred3, by = "id")
idall.2 <- idall.2[,c(1:307, 326:44976, 308:325)]

OB2 <- data.frame(matrix(NA, nrow = 303, ncol = 18))
colnames(OB2) <- paste("O",1:18, sep ="")
total2 <- 0

## find the new independent variable X1 and dependent Y1
q4 <- idall.2[,2:44958]
X2 <- model.matrix(~. , q4)[, -1]
idmissing.2 <- left_join(idmissing.1.id, pred3, by = "id")
#idmissing.2 <- idmissing.2[,c(107:108,2:106,109:307)]
idmissing.2 <- as.matrix(idmissing.2)[,-1]

Opred.2 <- function(X, Y, i){
  set.seed(10)
  fit.lasso <- cv.glmnet(X, Y, alpha = 1)
  fit.1 <- glmnet(X, Y, alpha = 1, lambda = fit.lasso$lambda.min)
  x <- predict(fit.1, idmissing.2, type = "response")
  a <- min(fit.lasso$cvm)
  list(cvm = a, predi = x)
}

for (i in 1:ncol(OB2)) {
  out <- Opred.2(X2, idob[, i], i)
  OB2[,i] <- out$predi
  total2 <- total2 + out$cvm
}
```